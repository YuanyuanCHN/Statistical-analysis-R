---
title: "Assignment6"
author: "Yuanyuan Pan"
date: "November 6, 2016"
output: pdf_document
---

## 1. Separate mixed samples of linear model data using logistic regression

In addition to the main sample `ResidualAnalysisProjectData_1.csv` you are given the training sample  `ResidualAnalysisProjectData_1_Train.csv` which indicates by which of the two mixed models each observation is generated (see column 3). Use this information to separate models in the main sample.

### 1.1. Analize the training sample
Read the training data.
```{r}
datapath = getwd()
LinearModel.Training<-read.csv(file=paste(datapath,'ResidualAnalysisProjectData_1_Train.csv',sep="/"),
                               header=TRUE,sep=",")
nSample.Training<-length(LinearModel.Training[,1])
head(LinearModel.Training)

```

Plot the training sample

```{r}
plot(LinearModel.Training[,1],LinearModel.Training[,2], type="p",pch=19)
```

Then separate the models in the training sample using the third column and plot the training subsamples:

```{r}
# Define training samples generated by model 1 and model 2
LinearModel.Training.1<-cbind(LinearModel.Training[,1],rep(NA,nSample.Training))
LinearModel.Training.2<-cbind(LinearModel.Training[,1],rep(NA,nSample.Training))
LinearModel.Training.1[LinearModel.Training[,3]*(1:nSample.Training),2]<-
  LinearModel.Training[LinearModel.Training[,3]*(1:nSample.Training),2]
LinearModel.Training.2[(1-LinearModel.Training[,3])*(1:nSample.Training),2]<-
  LinearModel.Training[(1-LinearModel.Training[,3])*(1:nSample.Training),2]

head(cbind(LinearModel.Training,
           Trainig1=LinearModel.Training.1[,2],
           Training2=LinearModel.Training.2[,2]))
# Plot the subsamples
matplot(LinearModel.Training[,1],cbind(LinearModel.Training.1[,2],LinearModel.Training.2[,2]),
        pch=16,col=c("green","blue"),ylab="Subsamples of the training sample")

```

Like in the project on analysis of residuals with `ResidualAnalysisProjectData_1.csv` estimate linear model for the training sample using function `lm`, look at the output of the function


```{r}
EstimatedLinearModel.Training <- lm(Output ~ Input, data = LinearModel.Training)
summary(EstimatedLinearModel.Training)$coefficients
summary(EstimatedLinearModel.Training)$r.squared
summary(EstimatedLinearModel.Training)$sigma

```

Interpret the results in the output. Compare the results for the trainig sample and the main sample from previous week: coefficients, $R^2$, $\sigma$.Observe the residuals of the training sample.


```{r}
EstimatedResiduals.Training<-EstimatedLinearModel.Training$residuals
plot(LinearModel.Training[,1],EstimatedResiduals.Training)
```

Using the third column of the training sample separate and plot the residuals for observations from different models

```{r}
# Define residuals corresponding to different models 
EstimatedResiduals.Training.1<-EstimatedResiduals.Training
EstimatedResiduals.Training.2<-EstimatedResiduals.Training
EstimatedResiduals.Training.1[(LinearModel.Training[,3]==0)*(1:nSample.Training)]<-NA
EstimatedResiduals.Training.2[(LinearModel.Training[,3]==1)*(1:nSample.Training)]<-NA
# Print the first ten columns to check the separation 
head(cbind(AllResiduals=EstimatedResiduals.Training,
      Training1Residuals=EstimatedResiduals.Training.1,
      Training2Residuals=EstimatedResiduals.Training.2,
      TrainingClass=LinearModel.Training[,3]))
# Plot the residuals corresponding to different models
matplot(LinearModel.Training[,1],cbind(EstimatedResiduals.Training.1,
                                       EstimatedResiduals.Training.2),
        pch=16,col=c("green","blue"),ylab="Separated parts of the training sample")
```

*What do you think about best way to separate the samples of residuals?*

### 1.2. Ligistic regression
Separate clusters of residuals using logistic regression with binary output from the third column and residuals of training sample as input.

Estimate logistic regression by calling `glm()` function.

Note: when we estimate logistic model we first create the data frame `Logistic.Model.Data` and use it in the call of `glm` as `glm(Logistic.Output~Logistic.Input,data=Logistic.Model.Data...` where Logistic.Input and Logistic.Output are the names of the data frame variables. This becomes important when we use the function predict later.

```{r}
# Create the data frame for logistic regression
Logistic.Model.Data<-data.frame(Logistic.Output=LinearModel.Training[,3],
                                Logistic.Input=EstimatedResiduals.Training)
LinearModel.Training.Logistic<-glm(Logistic.Output~Logistic.Input,data=Logistic.Model.Data,
                                   family=binomial(link=logit))
summary(LinearModel.Training.Logistic)

names(LinearModel.Training.Logistic)
```

*Interpret the summary of the model: what is the meaning and significance of coefficients.*

Define and plot probability of selecting an observation from the first model using predict function with type="response" argument. Plot the predicted probabilities

```{r}
Predicted.Probabilities.Training<-predict(LinearModel.Training.Logistic,type="response")
plot(LinearModel.Training[,1],Predicted.Probabilities.Training)
```

*How can we use this graph? What does it tell us?*

Classify the training sample using the estimated unscrambling sequence

```{r}
# Create the unscrambling sequence for the training sample
Unscrambling.Sequence.Training.Logistic<-
  (predict(LinearModel.Training.Logistic,type="response")>.5)*1
# Create classified residuals
ClassifiedResiduals.Training.1<-EstimatedResiduals.Training
ClassifiedResiduals.Training.2<-EstimatedResiduals.Training
ClassifiedResiduals.Training.1[(Unscrambling.Sequence.Training.Logistic==0)*
                                 (1:nSample.Training)]<-NA
ClassifiedResiduals.Training.2[(Unscrambling.Sequence.Training.Logistic==1)*
                                 (1:nSample.Training)]<-NA
head(cbind(AllTraining=EstimatedResiduals.Training,
           Training1=ClassifiedResiduals.Training.1,
           Training2=ClassifiedResiduals.Training.2))
# Plot both classes of the residuals
matplot(LinearModel.Training[,1],cbind(ClassifiedResiduals.Training.1,
                                       ClassifiedResiduals.Training.2),
        pch=16,col=c("green","blue"),ylab="Classified residuals, X-axis at 0")
axis(1,pos=0)

```

*Recall what classification rule we used in the previous assignment with these data?*
*What is the classification rule estimated by logistic regression?*

Recall what variable is the predictor of the model.

*Calculate classification boundary for the models using estimated coefficients of logistic regression.*


```{r}
Classification.Rule.Logistic <- 0.2044/5.2318 
Classification.Rule.Logistic
```

```{r}
matplot(LinearModel.Training[,1],cbind(ClassifiedResiduals.Training.1,
                                       ClassifiedResiduals.Training.2),
        pch=16,col=c("green","blue"),ylab="Classified residuals, X-axis at the rule level")
axis(1,pos=Classification.Rule.Logistic)
```

### 1.3. Separate subsamples in the main sample using the classifier trained on the training sample

Read the main sample.

```{r}
LinearModel<-read.csv(file=paste(datapath,'ResidualAnalysisProjectData_1.csv',sep="/"),header=TRUE,sep=",")
nSample<-length(LinearModel[,1])
head(LinearModel)

EstimatedLinearModel<-lm(LinearModel[,2]~LinearModel[,1])
EstimatedLinearModel$coefficients
EstimatedResiduals<-EstimatedLinearModel$residuals
plot(LinearModel[,1],EstimatedResiduals)
```

Define the predicted probabilities and separating sequence for the main sample.

Note: the function predict uses the argument newdata. This argument should be a data frame with the same names of variables as the data frame used to estimate logistic model. But the residuals in this data frame are from the main sample.

```{r}
Unscrambling.Sequence.Logistic<-(predict(LinearModel.Training.Logistic,
                                         newdata=data.frame(Logistic.Output=EstimatedResiduals,
                                                            Logistic.Input=EstimatedResiduals),
                                         type="response")>.5)*1
```

Estimate probability of the data classified as the first model.
Check the hypothesis p=0.5 against two-sided alternative.

```{r}
Probability<-sum(Unscrambling.Sequence.Logistic)/length(Unscrambling.Sequence.Logistic)
Probability
binom.test(sum(Unscrambling.Sequence.Logistic),length(Unscrambling.Sequence.Logistic), p=0.5, alternative = "two.sided")
```

What do you conclude based on the binomial test?

Classify the residuals of the main sample into 2 groups using the logistic model classifier. Plot them.

```{r}
# Create classified residuals
ClassifiedResiduals.1<-EstimatedResiduals
ClassifiedResiduals.2<-EstimatedResiduals
ClassifiedResiduals.1[(Unscrambling.Sequence.Logistic==0)*(1:nSample)]<-NA
ClassifiedResiduals.2[(Unscrambling.Sequence.Logistic==1)*(1:nSample)]<-NA
# Print first 10 rows to check
cbind(EstimatedResiduals,ClassifiedResiduals.1,ClassifiedResiduals.2)[1:10,]
##    EstimatedResiduals ClassifiedResiduals.1 ClassifiedResiduals.2

# Plot both classes of the residuals
matplot(LinearModel[,1],cbind(ClassifiedResiduals.1,
                              ClassifiedResiduals.2),
        pch=16,col=c("green","blue"),ylab="Classes of the main sample, X-axis at 0")
axis(1,pos=0)
```

The X axis on the plot above is located at zero level. The following graph shows it at the level of the classification rule estimated by logistic model.


```{r}
matplot(LinearModel[,1],cbind(ClassifiedResiduals.1,ClassifiedResiduals.2),
        pch=16,col=c("green","blue"),ylab="Classes of the main sample, X-axis at the rule level")
axis(1,pos=Classification.Rule.Logistic)
```

Separate the given sample into 2 subsamples using the trained logistic model.

```{r}
# Create recovered models
LinearModel1.Recovered<-LinearModel
LinearModel2.Recovered<-LinearModel
LinearModel1.Recovered[(1-Unscrambling.Sequence.Logistic)*(1:nSample),2]<-NA
LinearModel2.Recovered[Unscrambling.Sequence.Logistic*(1:nSample),2]<-NA
# Print the first 1 rows of scrambled and unscrambled samples
cbind(LinearModel,LinearModel1.Recovered,LinearModel2.Recovered)[1:10,]
# Plot the unscrambled subsamples
matplot(LinearModel[,1],cbind(LinearModel1.Recovered[,2],LinearModel2.Recovered[,2]), type="p",col=c("green","blue"),pch=19,ylab="Separated Subsamples")
```

Now estimate the linear models for the subsamples.

```{r}
LinearModel1.Recovered.lm<-lm(LinearModel1.Recovered[,2]~LinearModel1.Recovered[,1])
LinearModel2.Recovered.lm<-lm(LinearModel2.Recovered[,2]~LinearModel2.Recovered[,1])
```

Compare the results of fitting of the first recovered linear model

```{r}
summary(LinearModel1.Recovered.lm)
```

and the second recovered linear model:

```{r}
summary(LinearModel2.Recovered.lm)
```

*Compare the summaries of the mix with the summary of the single linear model fit.*

Plot the residuals estimated by a single linear model and the residuals of the unscrambled mix. Estimate standard deviations of the two samples of residuals:

```{r}
# Plot residuals
Residuals.Comparison<-cbind(Unscrambled.residuals=c(summary(LinearModel1.Recovered.lm)$residuals,summary(LinearModel2.Recovered.lm)$residuals),Single.Model.residuals=EstimatedResiduals)
matplot(Residuals.Comparison,type="p",pch=16,ylab="Residuals before and after unscrabling")
# Estimate standard deviations
apply(Residuals.Comparison,2,sd)
```

Conclusion

How the sample was mixed? With what probability?
Was the probability significantly different from 0.5?
What were the parameters of mixed models?
How much we reduced variance of residuals by separating the models?

Unscrumbling using logistic model gives not equivalent, but not significantly different results from our first heuristic approach.

## 2. Check Assumptions of Linear Model.

Read the data from file `Week6AssignmentData.csv`.
```{r}
assignmentData<-read.csv(file=paste(datapath,"AssignmentData.csv",sep="/"),
                         header = TRUE,sep=",")

head(assignmentData)

lm2 <- lm(Output ~ Input, data = assignmentData)

plot(lm2)
hist(lm2$residuals)
## Normal
hist(lm2$residuals)
ks.test(lm2$residuals,pnorm, alternative = "two.sided")

## IID
plot(lm2$residuals, type = "l")
residual_lm2  <- lm2$residuals
residual_lm2 <- (residual_lm2 - mean(residual_lm2))/sd(residual_lm2)
hist(residual_lm2)
qnorm(0)
Sample.histogram <- hist(pnorm(residual_lm2),freq = FALSE)
Sample.histogram.mean <- mean(Sample.histogram$density)
Sample.histogram.sd <- sd(Sample.histogram$density)
plot(Sample.histogram,freq=FALSE)
abline(h=Sample.histogram.mean)
abline(h=Sample.histogram.mean+1.96*Sample.histogram.sd,col="red",lty=2)
abline(h=Sample.histogram.mean-1.96*Sample.histogram.sd,col="red",lty=2)
## Autocorrelation function
acfï¼ˆlm2$residuals)

## Autocorrelation lag 1

lag1 <- residual_lm2[2:length(residual_lm2)]
Origin <- residual_lm2[1:(length(residual_lm2)-1)]
summary(lm(lag1 ~ Origin))

```

Are main assumptions of linear model satisfied?

* Gaussian assumption
* IID assumption
* Autocorrelation function
* Autocorrelation with lag 1
Hint. You might find assignment from week 2 useful.



## 3.Test

```{r}
dataPath <- getwd()
train_dat <- read.table(paste(dataPath,'Test_Sample_Train.csv',sep = '/'), header=TRUE)
main_dat <- read.table(paste(dataPath,'Test_Sample_Test.csv',sep = '/'), header=TRUE)
head(train_dat)
lm <- lm(Output ~ Input, data = train_dat )
residual_lm <- lm$residuals

lm_main <- lm(Output ~ Input, data = main_dat)
residual_main <- lm_main$residuals

Logistic.Model.Data <- data.frame(Logistic.Output = train_dat$Selection.Sequence, Logistic.Input = residual_lm)
LinearModel.Training.Logistic<-glm(Logistic.Output~Logistic.Input,data=Logistic.Model.Data,
                                   family=binomial(link=logit))
summary(LinearModel.Training.Logistic)
Unscrambling.Sequence.Logistic<-(predict(LinearModel.Training.Logistic,
                                         newdata=data.frame(Logistic.Output=residual_main,
                                                            Logistic.Input=residual_main),type="response")>.5)*1

res <- list(Unscrambling.Sequence.Logistic =  Unscrambling.Sequence.Logistic)
write.table(res, file = paste(dataPath,'result.csv',sep = '/'), row.names = F)
