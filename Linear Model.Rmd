---
title: "Linear Model"
author: "Yuanyuan Pan"
date: "October 15, 2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Week 3: Homework assignment: Simulation of 4 Linear Models

Set the values of slope (a) and intercept (b) parameters. Set the sample lengths to 1,000.

```{r}
# Slope and Intercept
a<-.8; b<-.1
nSample<-1000
```

### 1. Model 1

Simulate and plot Model1: input variable X ~ Norm(\(\mu=3,\sigma=2.5\)); model residuals Eps ~ Norm(\(\mu=0,\sigma=1.5\))
Use set.seed(111) to simulate \(X\) and set.seed(1112131415) to simulate Eps.

```{r}
set.seed(111)
X <- rnorm(nSample, 3, 2.5)
set.seed(1112131415)
Eps <- rnorm(nSample, 0, 1.5)
Y <- a*X + b + Eps
LinearModel1 <- as.data.frame(cbind(Y, X, Eps))
````

Check the simulated model


```{r}
head(LinearModel1)
plot(LinearModel1$X,LinearModel1$Y)
```

Plot the residuals of the model.

```{r}
plot(LinearModel1$Eps,type="l")
```

### 2. Model 2

Simulate and plot Model2: input variable X ~ Norm(\(\mu=3,\sigma=2.5\)); model residuals Eps ~ Unif(\(min=-4.33,max=4.33\)).
Use the same realization of \(X\) as in the first model.
Use the same seed set.seed(1112131415) to simulate Eps.
Plot the residuals of the model.

```{r}
set.seed(1112131415)
Eps <- runif(nSample, min = -4.33, max = 4.33)
Y <- a*X + b + Eps
LinearModel2 <- as.data.frame(cbind(Y, X, Eps))
head(LinearModel2)
plot(LinearModel2$X,LinearModel2$Y)
plot(LinearModel2$Eps,type="l")
```

### 3. Model 3

Simulate and plot Model3: input variable X ~ Norm^(\(\mu=3,\sigma=2.5\)); model residuals Eps ~ Cauchi*(\(location=0,scale=0.3\)).
Use the same realization of \(X\) as in the first model.
Use the same seed set.seed(1112131415) to simulate Eps.
Plot the residuals of the model.

```{r}
set.seed(1112131415)
Eps <- rcauchy(nSample, location = 0, scale = 0.3)
Y <- a*X + b + Eps
LinearModel3 <- as.data.frame(cbind(X, Y, Eps))
head(LinearModel3)
plot(LinearModel3$X,LinearModel2$Y)
plot(LinearModel3$Eps,type="l")
sd(LinearModel3$Eps)
```

Generate another 5 samples of residuals without any seed specification and estimate standard deviations for each of them.

```{r}
Eps1<-rcauchy(n=nSample,location=0,scale=.3)
Eps2<-rcauchy(n=nSample,location=0,scale=.3)
Eps3<-rcauchy(n=nSample,location=0,scale=.3)
Eps4<-rcauchy(n=nSample,location=0,scale=.3)
Eps5<-rcauchy(n=nSample,location=0,scale=.3)
c(sd(Eps1),sd(Eps2),sd(Eps3),sd(Eps4),sd(Eps5))
```

The standard deviations varies a lot from sample to sample, sometimes became extremely large.


### 4. Model 4

Simulate and plot Model4: input variable X ~ Norm(\(\mu=3,\sigma=2.5\)); model residuals Eps ~ a heteroscedastic process.
Use the same realization of \(X\) as in the first model.
Use the same seed.
Plot the residuals of the model.

Heteroscedasticity means that even though the model residuals Eps may be generated by a normal distribution, the standard deviation parameters for different sub samples are different.

Create the process of standard deviations in which the first 50 observations have sigma=2, followed by 75 observations with sigma=3.4, followed by 75 observations with sigma=0.8 and concluded by 50 observations with sigma=2.6.

Plot the trajectory of standard deviations of total length nSample=1000.
```{r}
sd.Values<-c(2,3.4,.8,2.6)
sd.process<-rep(c(rep(sd.Values[1],50),
                  rep(sd.Values[2],75),
                  rep(sd.Values[3],75),
                  rep(sd.Values[4],50)),
            4)
            
plot(sd.process,type="l")

```

Simulate the linear model residuals Eps with changing standard deviations.

```{r}
set.seed(1112131415);
Eps<-rnorm(nSample)*sd.process
```


Plot the residuals

```{r}
plot(Eps,type="l")
```

Observe how heteroscedasticity transforms normal distribution into leptokurtic distribution

```{r}
Xvariable<-(100*floor(min(Eps))):(100*ceiling(max(Eps)))
Xvariable<-Xvariable/100
# Plot the sample distribution and the theo. distribution
plot(Xvariable,dnorm(Xvariable,mean=mean(Eps),sd=sd(Eps)),type="l",
      ylim=c(0,.3),col="black",ylab="Distribution of Eps",xlab="")
lines(density(Eps),col="red")
```

Generate LinearModel4.
Plot it.

```{r}
Y<-a*X+b+Eps
LinearModel4<-as.data.frame(cbind(Y=Y,X=X))
plot(LinearModel4$X,LinearModel4$Y)
```

### 5. Effect of Residual Distribution on Correlation

Calculate the theoretical \(\rho^{2}\) for the “correct model” which is LinearModel1.
 \[Cov(X,Y) = a \sigma_x^2, \sigma_y^2 = a^2 \sigma_{x}^2 + \sigma_{\epsilon}^2.\] 
 \[\rho(X,Y) = \frac{Cov(X,Y)}{\sigma_x \sigma_y}\] \[\rho(X,Y)^2 = \frac{(a \sigma_x)^2}{(a \sigma_x)^2+\sigma^2_{\epsilon}}\]

```{r}
# Theoretical Rho^2
Theoretical.Rho.Squared<-(a*sd(LinearModel1$X))^2/((a*sd(LinearModel1$X))^2+sd(LinearModel1$Eps)^2)
Theoretical.Rho.Squared
```

And compare with the estimated \(\rho^{2}\)

```{r}
c(cor(LinearModel1$X,LinearModel1$Y)^2,
  cor(LinearModel2$X,LinearModel2$Y)^2,
  cor(LinearModel3$X,LinearModel3$Y)^2,
  cor(LinearModel4$X,LinearModel4$Y)^2)
```

**How do you interpret the results?**

### 6. Estimation of Linear Model

Estimate parameters \(a, b, \sigma\) using the function lm()

```{r}
m1<-lm(Y~X,data=LinearModel1)
summary(m1)
names(summary(m1))
summary(m1)$r.squared
summary(m1)$coeff
summary(m1)$sigma^2
var(summary(m1)$residuals)
```

Note the difference between summary(m1)$sigma^2 and var(summary(m1)$residuals).
The estimate returned by the model is \[\hat{\sigma}_{\epsilon,1}^2 = \frac{\sum_{i=1}^N residuals^2}{df} = \frac{\sum_{i=1}^N residuals^2}{N-2}.\]
But var() does not know anything about the model and its df. It estimates variance as \[\hat{\sigma}_{\epsilon,2}^2 = \frac{\sum_{i=1}^N residuals^2}{N-1}.\]
The two estimates are reconciled by: \[\hat{\sigma}_{\epsilon,1}^2 = \hat{\sigma}_{\epsilon,2}^2 \frac{N-1}{N-2}\]

```{r}
var(summary(m1)$residuals)*999/998
```

Estimate the same parameters using the method of moments directly.

```{r}
aEstimate <- cor(LinearModel1$X, LinearModel1$Y)*sd(LinearModel1$Y)/sd(LinearModel1$X)
bEstimate <- mean(LinearModel1$Y) - aEstimate*mean(LinearModel1$X)
sigmaEstimate <- sqrt(sd(LinearModel1$Y)^2-aEstimate^2*sd(LinearModel1$X)^2)

```

The result of estimation by method of moments is:

```{r}
c(aEstimate,bEstimate,sigmaEstimate)
```

Reconcile sigmaEstimate with m1$sigma.

```{r}
c(sigmaMetodMoments=sigmaEstimate,sigmaLinearModel=summary(m1)$sigma)
```

### 7. Fit lm() to the the Rest of Linear Models
Compare the differences between the assumptions of the 4 models and tell how they change the model behavior and estimated parameters.

```{r}
m2 <- lm(LinearModel2$Y ~ LinearModel2$X)
m3 <- lm(LinearModel3$Y ~ LinearModel3$X)
m4 <- lm(LinearModel4$Y ~ LinearModel4$X)
summary(m2)$coeff
summary(m2)$sigma
summary(m2)$r.squared
summary(m2)$df
summary(m3)$coeff
summary(m3)$sigma
summary(m3)$r.squared
summary(m3)$df
summary(m4)$coeff
summary(m4)$sigma
summary(m4)$r.squared
summary(m4)$df
```

### Test 3

```{r}
df <- read.table('Week3_Test_Sample.csv', header = TRUE)
Model <- lm(df$Y ~ df$X)
summary(Model)$coeff
mean(Model$residuals)
sd(Model$residuals)
hist(Model$residuals)
plot(Model$residuals)
```






